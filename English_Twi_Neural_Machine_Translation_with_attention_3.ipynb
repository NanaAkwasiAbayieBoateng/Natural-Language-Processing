{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "English_Twi_Neural_Machine_Translation_with_attention_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NanaAkwasiAbayieBoateng/Natural-Language-Processing/blob/master/English_Twi_Neural_Machine_Translation_with_attention_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zY7TrNFZTHH",
        "colab_type": "code",
        "outputId": "402c31a3-3070-4f11-eea8-88dcb3dae256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#specify tensorflow version to use\n",
        "%tensorflow_version 2.x\n",
        "#load tensorboard\n",
        "#%load_ext tensorboard\n",
        " #%tensorboard --logdir logs\n",
        "import tensorflow as tf\n",
        "%autosave 5\n",
        "#gpu_options = tf.GPUOptions(allow_growth=True)\n",
        "#session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
        "#session = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction)\n",
        "#from tf.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "IPython.notebook.set_autosave_interval(5000)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Autosaving every 5 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt4BurtWjtEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#drive.flush_and_unmount()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPp9jzVtY0DK",
        "colab_type": "code",
        "outputId": "3a04006c-8b60-46db-d127-24185b9964b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCiCUpTpSLC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#If you want to use the file and not have to remember to close it afterward, do this:\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/NaturalLanguageProcessing/GhanaNLP/jw300.en-tw.tw', 'r') as f:\n",
        "     #Twi_data = f.readlines()\n",
        "      Twi_data= f.read().splitlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZL_76fsaerK",
        "colab_type": "code",
        "outputId": "751349a2-5443-4e4a-dd6c-14cd8ead6732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "Twi_data[0:5] "
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['“ Oo , Yehowa , Boa Me Babea Kumaa Yi Ma Onni Nokware ! ”',\n",
              " 'WƆWOO me too abusua a wonim adwinne di mu wɔ Alsace , France , wɔ 1930 mu .',\n",
              " 'Ná Papa taa pa twere n’agua mu kenkan asase ho nsɛm anaa ewim nneɛma ho nhoma bi anwummere anwummere .',\n",
              " 'Ná me kraman da Papa nan so , na na ɔka nsɛntitiriw a epue wɔ n’akenkan no mu kyerɛ Maame bere a ɔnwene abusua no nneɛma no .',\n",
              " 'Ná m’ani gye anwummere a ɛtete saa no ho kɛse !']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HAkrCLGSK8y",
        "colab_type": "code",
        "outputId": "19798493-443f-479d-cb13-0b6f322e5612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "english_data = [line.rstrip() for line in open('/content/drive/My Drive/NaturalLanguageProcessing/GhanaNLP/jw300.en-tw.en')]\n",
        "english_data[0:5]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['“ Oh , Jehovah , Keep My Young Girl Faithful ! ”',\n",
              " 'I WAS born in 1930 in Alsace , France , into an artistic family .',\n",
              " 'During the evenings , Father , sitting in his lounge chair , would be reading some books about geography or astronomy .',\n",
              " 'My doggy would be sleeping by his feet , and Daddy would be sharing with Mum some highlights from his reading while she was knitting for her family .',\n",
              " 'How much I enjoyed those evenings !']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CIOAxoOw35p",
        "colab_type": "text"
      },
      "source": [
        "#### sequence to sequence\n",
        "encoder decoder model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxL44_zLw_rS",
        "colab_type": "code",
        "outputId": "21d3471f-5a73-45df-80a9-d9e1cd26de3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow_addons as tfa\n",
        "print(tfa.__version__)\n",
        "#print(dir(tfa.seq2seq))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnxXKDjq3jEL",
        "outputId": "d1ab2b6a-37f6-4af4-aab9-3fafbfccec4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "# encoding=utf8\n",
        "from importlib import reload\n",
        "import sys\n",
        "reload(sys)\n",
        "#sys.setdefaultencoding('utf8')\n",
        "%autosave 5"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "IPython.notebook.set_autosave_interval(5000)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Autosaving every 5 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rd0jw-eC3jEh",
        "colab": {}
      },
      "source": [
        "def normalize_eng(s):\n",
        "    s = unicode_to_ascii(s)\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s\n",
        "\n",
        "def normalize_twi(s):\n",
        "    s = unicode_to_ascii(s)\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s\n",
        "\n",
        "\n",
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence_english(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "  #w = re.sub(r'[^Ɔ-ɔɛƐ]+', r' ', w)\n",
        "  #strip() Parameters\n",
        "  #chars (optional) - a string specifying the set of characters to be removed.\n",
        "  #If the chars argument is not provided, all leading and trailing whitespaces are removed from the string.\n",
        "  w = w.rstrip().strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "def preprocess_sentence_twi(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-ZɛƐɔƆ?.!,¿]+\", \" \", w)\n",
        "  #w = re.sub(r'[^Ɔ-ɔɛƐ]+', r' ', w)\n",
        "  #strip() Parameters\n",
        "  #chars (optional) - a string specifying the set of characters to be removed.\n",
        "  #If the chars argument is not provided, all leading and trailing whitespaces are removed from the string.\n",
        "  w = w.rstrip().strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkLhlUiChXtN",
        "colab_type": "code",
        "outputId": "cb1449b3-825e-4009-ff0d-8f0a237a05af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "type(english_data)\n",
        "\n",
        "english_d  = list(map(preprocess_sentence_english,english_data))\n",
        "english_d[0:5]"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> oh , jehovah , keep my young girl faithful ! <end>',\n",
              " '<start> i was born in in alsace , france , into an artistic family . <end>',\n",
              " '<start> during the evenings , father , sitting in his lounge chair , would be reading some books about geography or astronomy . <end>',\n",
              " '<start> my doggy would be sleeping by his feet , and daddy would be sharing with mum some highlights from his reading while she was knitting for her family . <end>',\n",
              " '<start> how much i enjoyed those evenings ! <end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJw9rV8AwQ3U",
        "colab_type": "code",
        "outputId": "7f326a2e-b326-4cef-bac8-71ab263931fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "twi_d  = list(map(preprocess_sentence_twi,Twi_data))\n",
        "twi_d[0:5]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> oo , yehowa , boa me babea kumaa yi ma onni nokware ! <end>',\n",
              " '<start> wɔwoo me too abusua a wonim adwinne di mu wɔ alsace , france , wɔ mu . <end>',\n",
              " '<start> na papa taa pa twere n agua mu kenkan asase ho nsɛm anaa ewim nneɛma ho nhoma bi anwummere anwummere . <end>',\n",
              " '<start> na me kraman da papa nan so , na na ɔka nsɛntitiriw a epue wɔ n akenkan no mu kyerɛ maame bere a ɔnwene abusua no nneɛma no . <end>',\n",
              " '<start> na m ani gye anwummere a ɛtete saa no ho kɛse ! <end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-mhvNEJ8UYL",
        "colab_type": "code",
        "outputId": "f18e23bb-ca06-464b-e696-ec04ac888b81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "\n",
        "all_data= (twi_d[0:30000]),(english_d[0:30000])\n",
        "all_data[0][0:5]"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> oo , yehowa , boa me babea kumaa yi ma onni nokware ! <end>',\n",
              " '<start> wɔwoo me too abusua a wonim adwinne di mu wɔ alsace , france , wɔ mu . <end>',\n",
              " '<start> na papa taa pa twere n agua mu kenkan asase ho nsɛm anaa ewim nneɛma ho nhoma bi anwummere anwummere . <end>',\n",
              " '<start> na me kraman da papa nan so , na na ɔka nsɛntitiriw a epue wɔ n akenkan no mu kyerɛ maame bere a ɔnwene abusua no nneɛma no . <end>',\n",
              " '<start> na m ani gye anwummere a ɛtete saa no ho kɛse ! <end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSwJEOpTsTNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset_eng(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence_english(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPoajtCJtd7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#encoding = ‘utf-8-sig’ is added to overcome the issue when exporting ‘Non-English’ languages.\n",
        "def create_dataset_twi(path, num_examples):\n",
        "  lines = io.open(path, encoding='utf-8-sig').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence_twi(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6UOWPlCsoSp",
        "colab_type": "code",
        "outputId": "04b9b4fb-7dea-427f-cf52-9548e084c9c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_examples=3000\n",
        "path_eng='/content/drive/My Drive/NaturalLanguageProcessing/GhanaNLP/jw300.en-tw.en' \n",
        "\n",
        "eng=create_dataset_eng(path_eng, num_examples)\n",
        "print(eng)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<zip object at 0x7f72d065c2c8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FkRW8pOtHu1",
        "colab_type": "code",
        "outputId": "4cb45c89-1f16-45d9-d590-f2d819b91ea9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path_twi = '/content/drive/My Drive/NaturalLanguageProcessing/GhanaNLP/jw300.en-tw.tw'\n",
        "twi=create_dataset_twi(path_twi, num_examples)\n",
        "print(twi)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<zip object at 0x7f72ce5fd2c8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OmMZQpdO60dt",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "  return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bIOn8RCNDJXG",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAMb7gF8eOf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data( num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang  = english_d[0:num_examples]\n",
        "  inp_lang =   twi_d[0:num_examples]\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld1b5ps7yy9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 3000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_data( num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQKYLwl0Gh5V",
        "colab_type": "code",
        "outputId": "dc6ad2a7-fb7f-4cf4-c0be-b4607bc7c203",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(max_length_targ)\n",
        "print(max_length_inp)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "113\n",
            "150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6364680f-9cb4-4352-d042-cb884178e22b",
        "id": "Ary6bwnA4dQc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2400 2400 600 600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lJPmLZGMeD5q",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VXukARTDd7MT",
        "outputId": "7501a5bf-2cfc-4093-ac7b-5fd7c3354f0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "3 ----> <start>\n",
            "229 ----> awiei\n",
            "93 ----> koraa\n",
            "12 ----> wɔ\n",
            "1248 ----> september\n",
            "2 ----> no\n",
            "6 ----> ,\n",
            "74 ----> ɔde\n",
            "292 ----> too\n",
            "163 ----> paapa\n",
            "134 ----> anim\n",
            "7 ----> sɛ\n",
            "6 ----> ,\n",
            "7 ----> sɛ\n",
            "4107 ----> wannyae\n",
            "10 ----> ne\n",
            "194 ----> kristofo\n",
            "242 ----> gyidi\n",
            "2 ----> no\n",
            "9 ----> mu\n",
            "1 ----> a\n",
            "6 ----> ,\n",
            "1768 ----> obefi\n",
            "132 ----> fie\n",
            "23 ----> hɔ\n",
            "5 ----> .\n",
            "4 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "469 ----> finally\n",
            "4 ----> ,\n",
            "10 ----> in\n",
            "1248 ----> september\n",
            "4 ----> ,\n",
            "43 ----> she\n",
            "1944 ----> confronted\n",
            "77 ----> him\n",
            "19 ----> with\n",
            "5 ----> the\n",
            "5251 ----> ultimatum\n",
            "11 ----> that\n",
            "21 ----> he\n",
            "145 ----> give\n",
            "79 ----> up\n",
            "5 ----> the\n",
            "181 ----> christian\n",
            "384 ----> faith\n",
            "37 ----> or\n",
            "564 ----> else\n",
            "43 ----> she\n",
            "66 ----> would\n",
            "562 ----> leave\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TqHsArVZ3jFS",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "#BATCH_SIZE = 32\n",
        "#for step in xrange(test_size / BATCH_SIZE):\n",
        "#for step in xrange(test_size // BATCH_SIZE):\n",
        "#In order to pass a int instead of a float to xrange and avoid a runtime exception.\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qc6-NK1GtWQt",
        "outputId": "a141bbcb-6baa-4fa4-d785-c4fe6d9ef04d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 150]), TensorShape([64, 113]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nZ2rI24i3jFg",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60gSVh05Jl6l",
        "outputId": "c7253635-03ff-490e-c0ad-1c3c05117fb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 150, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "umohpBN2OM94",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k534zTHiDjQU",
        "outputId": "db3eb0be-9785-413a-800e-f8f2df96b474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 150, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ_B3mhW3jFk",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P5UY8wko3jFp",
        "outputId": "cdd68ac1-b06b-4285-af06-fede682b1267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 6521)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmTHr5iV3jFr",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zj8bXQTgNwrF",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sC9ArXSsVfqn",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ddefjBMa3jF0",
        "outputId": "fa7173f0-39af-4794-94fe-4389ca7e8a15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.4427\n",
            "Epoch 1 Loss 1.0952\n",
            "Time taken for 1 epoch 610.827437877655 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.0101\n",
            "Epoch 2 Loss 1.0041\n",
            "Time taken for 1 epoch 450.0042235851288 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.9739\n",
            "Epoch 3 Loss 0.9684\n",
            "Time taken for 1 epoch 451.62801146507263 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.0080\n",
            "Epoch 4 Loss 0.9505\n",
            "Time taken for 1 epoch 451.20018339157104 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.8565\n",
            "Epoch 5 Loss 0.9346\n",
            "Time taken for 1 epoch 449.37496280670166 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.8838\n",
            "Epoch 6 Loss 0.9206\n",
            "Time taken for 1 epoch 445.0381245613098 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.7593\n",
            "Epoch 7 Loss 0.8989\n",
            "Time taken for 1 epoch 449.1849672794342 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.8840\n",
            "Epoch 8 Loss 0.8768\n",
            "Time taken for 1 epoch 449.5388386249542 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.9021\n",
            "Epoch 9 Loss 0.8498\n",
            "Time taken for 1 epoch 441.9685733318329 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.7663\n",
            "Epoch 10 Loss 0.8190\n",
            "Time taken for 1 epoch 442.37912774086 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EbQpyYs13jF_",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence_twi(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s5hQWlbN3jGF",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sl9zUHzg3jGI",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UJpT9D5_OgP6",
        "outputId": "546f4fa6-4bf2-4ba4-bedd-9743ac9c9708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f72868bdbe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WrAM0FDomq3E",
        "outputId": "c5f43b12-3851-47a9-a0d4-f86b7cd74341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        }
      },
      "source": [
        "#translate(u'hace mucho frio aqui.')\n",
        "translate(u'kristo nkutoo ne yɛn gyefo .')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> kristo nkutoo ne yɛn gyefo . <end>\n",
            "Predicted translation: i was a bible . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}